# src/train_models.py
import pandas as pd
import numpy as np
import joblib
import os
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
# ä¿®æ­£é€™è£¡ï¼šè£œä¸Š RandomizedSearchCV
from sklearn.model_selection import train_test_split, RandomizedSearchCV
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout, Input, BatchNormalization
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PROCESSED_DATA_PATH = os.path.join(BASE_DIR, 'data', 'processed_data.csv')
MODEL_DIR = os.path.join(BASE_DIR, 'models')

if not os.path.exists(MODEL_DIR): os.makedirs(MODEL_DIR)

def train():
    print("æ­£åœ¨è¼‰å…¥è¨“ç·´è³‡æ–™...")
    if not os.path.exists(PROCESSED_DATA_PATH):
        print("âŒ è«‹å…ˆåŸ·è¡Œ data_processing.py")
        return
        
    df = pd.read_csv(PROCESSED_DATA_PATH)
    
    # --- ç‰¹å¾µå·¥ç¨‹ ---
    # ç¢ºä¿åˆ†æ¯ä¸ç‚º 0
    df['home_pyth'] = (df['home_roll_b_r']**1.83) / ((df['home_roll_b_r']**1.83 + df['home_roll_p_r']**1.83) + 1e-9)
    df['vis_pyth'] = (df['vis_roll_b_r']**1.83) / ((df['vis_roll_b_r']**1.83 + df['vis_roll_p_r']**1.83) + 1e-9)
    
    df['diff_win_rate'] = df['home_pre_win_rate'] - df['vis_pre_win_rate']
    df['diff_pyth'] = df['home_pyth'] - df['vis_pyth']
    df['diff_run_diff'] = (df['home_roll_b_r'] - df['home_roll_p_r']) - (df['vis_roll_b_r'] - df['vis_roll_p_r'])
    
    # ç¢ºä¿æŠ•æ‰‹æ¬„ä½å­˜åœ¨
    if 'home_sp_era' not in df.columns: df['home_sp_era'] = 4.5
    if 'vis_sp_era' not in df.columns: df['vis_sp_era'] = 4.5
    if 'home_sp_whip' not in df.columns: df['home_sp_whip'] = 1.35
    if 'vis_sp_whip' not in df.columns: df['vis_sp_whip'] = 1.35
    
    df['diff_sp_era'] = df['home_sp_era'] - df['vis_sp_era']
    df['diff_sp_whip'] = df['home_sp_whip'] - df['vis_sp_whip']
    
    # ç¢ºä¿ EqA æ¬„ä½å­˜åœ¨
    if 'home_roll_b_eqa' not in df.columns: df['home_roll_b_eqa'] = 0.250
    if 'vis_roll_b_eqa' not in df.columns: df['vis_roll_b_eqa'] = 0.250
    
    df['diff_eqa'] = df['home_roll_b_eqa'] - df['vis_roll_b_eqa']
    
    df = df.replace([np.inf, -np.inf], 0).fillna(0)
    
    features = [
        'diff_win_rate', 'diff_pyth', 'diff_run_diff', 
        'diff_sp_era', 'diff_sp_whip', 'diff_eqa',
        'home_pre_win_rate', 'vis_pre_win_rate',
        'home_pyth', 'vis_pyth',
        'home_roll_b_r', 'vis_roll_b_r',
        'home_roll_p_r', 'vis_roll_p_r',
        'home_sp_era', 'vis_sp_era',
        'home_sp_whip', 'vis_sp_whip',
        'home_roll_b_eqa', 'vis_roll_b_eqa'
    ]
    target = 'home_target'
    
    # åˆ†å‰² (ä½¿ç”¨ stratify ç¢ºä¿å‹è² æ¯”ä¾‹ä¸€è‡´)
    train_df, test_df = train_test_split(df, test_size=0.1, random_state=42, shuffle=True, stratify=df[target])
    
    X_train = train_df[features]
    y_train = train_df[target]
    X_test = test_df[features]
    y_test = test_df[target]
    
    print(f"ç‰¹å¾µæ•¸é‡: {len(features)}")
    print(f"è¨“ç·´é›†: {len(X_train)}, æ¸¬è©¦é›†: {len(X_test)}")
    
    # æ¨™æº–åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    joblib.dump(scaler, os.path.join(MODEL_DIR, 'scaler.pkl'))
    
    # --- 1. XGBoost (ä½¿ç”¨ RandomizedSearchCV å°‹æ‰¾æœ€ä½³åƒæ•¸) ---
    print("\nğŸš€ æ­£åœ¨æœ€ä½³åŒ– XGBoost åƒæ•¸ (é€™éœ€è¦å¹¾åˆ†é˜)...")
    
    xgb_param_dist = {
        'n_estimators': [100, 300, 500, 800],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 4, 5, 6],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9],
        'gamma': [0, 0.1, 0.2],
        'min_child_weight': [1, 3, 5]
    }
    
    xgb = XGBClassifier(eval_metric='logloss', random_state=42)
    
    # éš¨æ©Ÿæœå°‹ 20 çµ„åƒæ•¸
    search = RandomizedSearchCV(
        estimator=xgb,
        param_distributions=xgb_param_dist,
        n_iter=20,
        scoring='accuracy',
        cv=3,
        verbose=1,
        n_jobs=-1,
        random_state=42
    )
    
    search.fit(X_train, y_train)
    best_xgb = search.best_estimator_
    
    print(f"âœ… XGBoost æœ€ä½³åƒæ•¸: {search.best_params_}")
    print(f"âœ… XGBoost æœ€ä½³é©—è­‰åˆ†æ•¸: {search.best_score_:.4f}")
    
    y_pred_xgb = best_xgb.predict(X_test)
    acc_xgb = accuracy_score(y_test, y_pred_xgb)
    print(f"ğŸ† XGBoost æ¸¬è©¦é›†æ­£ç¢ºç‡: {acc_xgb:.4f}")
    joblib.dump(best_xgb, os.path.join(MODEL_DIR, 'xgb_model.pkl'))

    # --- 2. Random Forest (ç°¡å–®åƒæ•¸èª¿æ•´) ---
    print("\nğŸŒ² è¨“ç·´ Random Forest...")
    rf = RandomForestClassifier(n_estimators=300, max_depth=12, min_samples_split=5, random_state=42)
    rf.fit(X_train, y_train)
    print(f"RF Accuracy: {accuracy_score(y_test, rf.predict(X_test)):.4f}")
    joblib.dump(rf, os.path.join(MODEL_DIR, 'rf_model.pkl'))
    
    # --- 3. Keras (æ·±åº¦å­¸ç¿’å„ªåŒ–ç‰ˆ) ---
    print("\nğŸ§  è¨“ç·´ Keras Neural Network (å„ªåŒ–ç‰ˆ)...")
    # åŠ å…¥ BatchNormalization å’Œ EarlyStopping é˜²æ­¢éæ“¬åˆ
    model = Sequential([
        Input(shape=(len(features),)),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.4),
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), 
                  loss='binary_crossentropy', 
                  metrics=['accuracy'])
    
    # æ—©åœæ©Ÿåˆ¶ï¼šå¦‚æœé©—è­‰é›†æº–ç¢ºç‡ 10 æ¬¡æ²’æå‡å°±åœæ­¢è¨“ç·´
    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    # è‡ªå‹•é™ä½å­¸ç¿’ç‡
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)

    model.fit(
        X_train_scaled, y_train, 
        epochs=100, 
        batch_size=64, 
        validation_split=0.2,
        callbacks=[early_stop, reduce_lr],
        verbose=0
    )
    
    _, acc_keras = model.evaluate(X_test_scaled, y_test, verbose=0)
    print(f"ğŸ† Keras Accuracy: {acc_keras:.4f}")
    model.save(os.path.join(MODEL_DIR, 'keras_model.h5'))
    
    print("\n=== è¨“ç·´å®Œæˆ ===")

if __name__ == "__main__":
    train()